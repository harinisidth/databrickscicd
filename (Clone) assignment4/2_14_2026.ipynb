{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db8a202f-7016-48ec-b4c4-da60e727b370",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You are given a dataset of employees in an Indian company with columns: emp_id, name, department, salary, and city. Write a PySpark program to find the total salary paid in each department.\n",
    "Sample Data:\n",
    "emp_id, name, department, salary, city\n",
    "101, Rajesh, IT, 75000, Bangalore\n",
    "102, Priya, HR, 60000, Mumbai\n",
    "103, Anil, IT, 80000, Hyderabad\n",
    "104, Sneha, HR, 62000, Pune\n",
    "105, Manish, Finance, 90000, Chennai\n",
    "106, Suresh, IT, 78000, Bangalore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d51a24ce-8d85-49d1-9de2-fb9e317be9c2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (101, \"Rajesh\", \"IT\", 75000, \"Bangalore\"),\n",
    "    (102, \"Priya\", \"HR\", 60000, \"Mumbai\"),\n",
    "    (103, \"Anil\", \"IT\", 80000, \"Hyderabad\"),\n",
    "    (104, \"Sneha\", \"HR\", 62000, \"Pune\"),\n",
    "    (105, \"Manish\", \"Finance\", 90000, \"Chennai\"),\n",
    "    (106, \"Suresh\", \"IT\", 78000, \"Bangalore\")\n",
    "]\n",
    "col = [\"emp_id\", \"name\", \"department\", \"salary\", \"city\"]\n",
    "df = spark.createDataFrame(data, col)\n",
    "df.createOrReplaceTempView(\"employee\")\n",
    "df1 = spark.sql(\"SELECT department, SUM(salary) AS total_salary FROM employee GROUP BY department\")\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9366bc24-50fa-41a3-8ae0-61bd1eb87a1c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "PySpark groupBy sum fix"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df1 = df.groupBy(\"department\").agg(sum(\"salary\").alias(\"total_salary\"))\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edfa11ec-9d24-46fc-b806-1d51aaa5dbdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You are given a dataset containing customer transactions in an Indian e-commerce platform with columns: cust_id, cust_name, city, purchase_amount, and product_category. Some records have missing purchase_amount. Write a PySpark program to fill missing purchase_amount values with the average purchase amount of that product category.\n",
    "\n",
    "Sample Data:\n",
    "cust_id, cust_name, city, purchase_amount, product_category\n",
    "201, Aman, Delhi, 1500, Electronics\n",
    "202, Kiran, Mumbai, , Fashion\n",
    "203, Ravi, Bangalore, 2000, Electronics\n",
    "204, Simran, Hyderabad, , Fashion\n",
    "205, Vinay, Pune, 1800, Electronics\n",
    "206, Pooja, Chennai, 1300, Grocery\n",
    "Expected Output (assuming average for Fashion = 2000):\n",
    "201, Aman, Delhi, 1500, Electronics\n",
    "202, Kiran, Mumbai, 2000, Fashion\n",
    "203, Ravi, Bangalore, 2000, Electronics\n",
    "204, Simran, Hyderabad, 2000, Fashion\n",
    "205, Vinay, Pune, 1800, Electronics\n",
    "\n",
    "206, Pooja, Chennai, 1300, Grocery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4c56c7-2881-4b5a-bb7c-3ca455cefa61",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fix groupBy column reference"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import avg, when, col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "data = [\n",
    "    (201, \"Aman\", \"Delhi\", 1500, \"Electronics\"),\n",
    "    (202, \"Kiran\", \"Mumbai\", None, \"Fashion\"),\n",
    "    (203, \"Ravi\", \"Bangalore\", 2000, \"Electronics\"),\n",
    "    (204, \"Simran\", \"Hyderabad\", 2000, \"Fashion\"),\n",
    "    (205, \"Vinay\", \"Pune\", \" \", \"Electronics\"),\n",
    "    (206, \"Pooja\", \"Chennai\", 1300, \"Grocery\")\n",
    "]\n",
    "\n",
    "schema=StructType(\n",
    "   [ StructField(\"cust_id\",StringType(),False),\n",
    "    StructField(\"cust_name\",StringType(),False),\n",
    "    StructField(\"city\",StringType(),False),\n",
    "    StructField(\"purchase_amount\",StringType(),True),\n",
    "    StructField(\"product_category\",StringType(),False)]\n",
    ")\n",
    "df=spark.createDataFrame(data, schema)\n",
    "df_clean = df.withColumn(\"purchase_amount\", when((col(\"purchase_amount\").isNull()) | (col(\"purchase_amount\") == \" \"), None).otherwise(col(\"purchase_amount\")))\n",
    "df_clean = df_clean.withColumn(\"purchase_amount\", col(\"purchase_amount\").cast(\"double\"))\n",
    "df1=df_clean.groupBy(\"product_category\").agg(avg(\"purchase_amount\").alias(\"avg_purchase\"))\n",
    "df2=df_clean.join(df1,\"product_category\",\"left\")\n",
    "df3=df2.withColumn(\"purchase_amount\", when(col(\"purchase_amount\").isNull(), col(\"avg_purchase\")).otherwise(col(\"purchase_amount\"))).drop(\"avg_purchase\")\n",
    "display(df3)\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a42e58c-f21c-4c56-9530-6350065b4a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You have a dataset of students from different Indian states with columns: student_id, student_name, state, score. Write a PySpark program to rank students within each state based on their scores in descending order.\n",
    "Sample Data:\n",
    "student_id, student_name, state, score\n",
    "301, Rohit, Maharashtra, 85\n",
    "302, Sneha, Karnataka, 92\n",
    "303, Amit, Maharashtra, 90\n",
    "304, Kunal, Karnataka, 88\n",
    "305, Nidhi, Maharashtra, 78\n",
    "306, Pavan, Karnataka, 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba36bcf-7e34-4c97-862f-03f4abe2d100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window as w\n",
    "data = [\n",
    "    (301, \"Rohit\", \"Maharashtra\", 85),\n",
    "    (302, \"Sneha\", \"Karnataka\", 92),\n",
    "    (303, \"Amit\", \"Maharashtra\", 90),\n",
    "    (304, \"Kunal\", \"Karnataka\", 88),\n",
    "    (305, \"Nidhi\", \"Maharashtra\", 78),\n",
    "    (306, \"Pavan\", \"Karnataka\", 80)\n",
    "]\n",
    "schema=StructType([\n",
    "    StructField(\"student_id\", StringType(), False),\n",
    "    StructField(\"student_name\", StringType(), False),\n",
    "    StructField(\"state\", StringType(), False),\n",
    "    StructField(\"marks\", StringType(), False)\n",
    "])\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df.createOrReplaceTempView(\"Student\")\n",
    "df=spark.sql('select * from (select *,dense_rank() over(partition by state order by marks desc,state asc) as rn from Student) t ')\n",
    "display(df)\n",
    "windowSpec=Window.partitionBy(\"state\").orderBy(col(\"marks\").desc())\n",
    "df1=df.withColumn(\"rn\",F.dense_rank().over(windowSpec))\n",
    "df1.display()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "49aa15b2-d20c-4753-b769-2ae8f01cf82c",
     "origId": 1277423550106201,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_14_2026",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
